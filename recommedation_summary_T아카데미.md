# 추천시스템 - T아카데미
## 정의
추천시스템은 사용자에게 상품을 제안하는 소프트웨어 도구 이자 기술입니다. 이러한 제안은 어떤 상품을 구매할지, 어떤 음악을 들을지 또는 어떤 온라인 뉴스를 읽을지와 같은 다양한 의사결정과 연관있습니다.

## 목표
어떤 사용자에게 어떤 상품을 어떻게 추천할지에 대해 이해

### 파레토의 법칙
- 상위 20%가 80%의 가치를 창출한다
### 롱테일의 법칙
- 하위 80%가 상위 20%의 가치보다 크다

### 연관분석(Association Analysis)
- 룰기반의 모델로서 상품과 상품 사이에 어떤 연관이 있는지 찾아내는 알고리즘
- 2가지 형태로 존재
	1. 얼마나(frequent) 같이 구매가 되는가?
	2. A 아이템을 구매하는 사람이 B 아이템을 구매하는가?
- 어떤 상품들이 한 장바구니 안에 담기는지 살피는 모습과 비슷하기 때문에 장바구니 분석이라고 표현하기도 함
- 평가지표
	- support(지지도): support(A) = P(A)
	- lift(향상도): 두 사건이 동시에 얼마나 발생하는지 비율, 독립성을 측정 lift(A->B) = P(A,B) / P(A)P(B)
	- confidence(신뢰도): confidence(A->B) = P(A,B) / P(A)
- 규칙 생성
	- 가능한 모든 경우의 수를 탐색해서 지지도, 신뢰도, 향상도가 높은 규칙들을 찾아내는 방식
	- 상품이 4개일 때, 전체 경우의 수
	- 4C1: 4
	- 4C2: 6
	- 4C3: 4
	- 4C4: 1
	- 전체 경우의 수 = 15
- 문제점
	- 아이템의 증가에 따른 규칙의 수의 증가가 기하급수적으로 증가
	- 아이템이 100개인 경우에는 규칙의 수가 1.26*10^30

### Apriori 알고리즘
- 아이템셋의 증가를 줄이기 위한 방법
- 기본적인 아이디어는 "빈번한 아이템셋은 하위 아이템셋 또한 빈번할 것이다" 즉, "빈번하지 않은 아이템셋은 하위 아이템셋 또한 빈번하지 않다"를 이용해서 아이템셋의 증가를 줄이는 방법
- 아이디어
	- {2,3}의 지지도 > {0,2,3}, {1,2,3}의 지지도
	- P(item 2, item 3) > P(item 0, item 2, item 3)
	- P(item 2, item 3) > P(item 1, item 2, item 3)
1. k개의 item을 가지고 단일항목집단 생성(one-item frequent set)
2. 단일항목집단에서 최소 지지도 이상의 항목만 선택
3. 2에서 선택된 항목만을 대상으로 2개 항목집단 생성
4. 2개 항목집단에서 최소 지지도 혹은 신뢰도 이상의 항목만 선택
5. 위의 과정을 k개의 k-item frequent set을 생성할 때까지 반복
- 장점
	- 원리가 간단하여 사용자가 쉽게 이해할 수 있고 의미를 파악할 수 있음
	- 유의한 연관성을 갖는 구매패턴을 찾아줌
- 단점
	- 데이터가 클 경우(item이 많은 경우)에 속도가 느리고 연산량이 많음
	- 실제 사용시에 많은 연관상품들이 나타나는 단점이 있음

### FP-Growth
- 원리
1. 모든 거래를 확인하여, 각 아이템마다의 지지도(support)를 계산하고 최소 지지도 이상의 아이템만 선택
2. 모든 거래에서 빈도가 높은 아이템 순서대로 순서를 정렬
3. 부모 노드를 중심으로 거래를 자식노드로 추가해주면서 tree를 생성
4. 새로운 아이템이 나올 경우에는 부모노드로부터 시작하고, 그렇지 않으면 기존의 노드에서 확장
5. 지지도가 낮은 순서부터 시작하여, 조건부 패턴을 생성
6. 모든 아이템에 대해서 반복
7. Conditional Pattern bases를 기반으로 패턴 생성
- 장점
	- Apriori 알고리즘보다 빠르고 2번의 탐색만 필요로 함
	- 후보 Itemsets을 생성할 필요없이 진행 가능
- 단점
	- 대용량의 데이터셋에서 메모리를 효율적으로 사용하지 않음
	- Apriori 알고리즘에 비해서 설계하기 어려움
	- 지지도의 계산이 FP-Tree가 만들어지고 나서야 가능함

### 컨텐츠 기반 모델
- 컨텐츠 기반 추천시스템은 사용자가 이전에 구매한 상품중에서 좋아하는 상품들과 유사한 상품들을 추천하는 방법
- Represented Items
	- Items을 벡터 형태로 표현, 도메인에 따라 다른 방법이 적용
	- 벡터들간의 유사도를 계산
	- 벡터 1부터 N까지 자신과 유사한 벡터를 추출
- 유사도 함수(상황마다 다른 함수 적용)
	- 유클리디안 유사도
		- 문서간의 유사도를 계산
		- 1/(유클리디안 거리 + 1e-0.5)
		- 계산하기 쉬움
		- p와 q의 분포가 다르거나 범위가 다른 경우에 상관성을 놓침
	- 코사인 유사도
		- 문서간의 유사도를 계산
		- 벡터의 크기가 중요하지 않은 경우에 거리를 측정하기 위한 메트릭으로 사용(예: 문서내에서 단어의 빈도수 - 문서들의 길이가 고르지 않더라도 문서내에서 얼마나 나왔는지라는 비율을 확인하기 때문에 상관없음)
		- 벡터의 크기가 중요한 경우에 대해서 잘 작동하지 않음
	- 피어슨 유사도
	- 자카드 유사도
- 장점
	- 협업필터링은 다른 사용자들의 평점이 필요한 반면에, 자신의 평점만을 가지고  추천시스템을 만들 수 있음
	- item의 feature를 통해서 추천을 하기에 추천이 된 이유를 설명하기 용이함
	- 사용자가 평점을 매기지 않은 새로운 item이 들어올 경우에도 추천이 가능함
- 단점
	- item의 feature을 추출해야 하고 이를 통해서 추천하기 때문에 제대로 feature을 추출하지 못하면 정확도가 낮음. 그렇기에 Domain Knowledge가 분석시에 필요할 수도 있음
	- 기존의 item과 유사한 item 위주로만 추천하기에 새로운 장르의 item을 추천하기 어려움
	- 새로운 사용자에 대해서 충분한 평점이 쌓이기 전까지는 추천하기 힘듬

### TF-IDF
- 특정 문서 내에 특정 단어가 얼마나 자주 등장하는 지를 의미하는 단어 빈도(TF)와 전체 문서에서 특정 단어가 얼마나 자주 등장하는 지를 의미하는 역문서 빈도(DF)를 통해서 "다른 문서에서는 등장하지 않지만 특정 문서에서만 자주 등장하는 단어"를 찾아서 문서 내 단어의 가중치를 계산하는 방법
- 용도로는 문서의 핵심어를 추출, 문서들 사이의 유사도를 계산, 검색 결과의 중요도를 정하는 작업 등에 활용
- 사용하는 이유?
	1. Item이라는 컨텐츠를 벡터로 "Feature Extract" 과정을 수행해준다
	2. 빈도수를 기반으로 많이 나오는 중요한 단어들을 잡아준다. 이러한 방법을 Counter Vectorizer라고 한다
	3. 하지만, Counter Vectorizer는 단순 빈도만을 계산하기에 조사, 관사처럼 의미는 없지만 문장에 많이 등장하는 단어들도 높게 쳐주는 한계가 있다. 이러한 단어들에는 패널티를 줘서 적절하게 중요한 단어만을 잡아내는게 TF-IDF 기법이다.
- TF(d,t): 특정 문서 d에서의 특정 단어 t의 등장 횟수
- DF(t): 특정 단어 t가 등장한 문서의 수
- IDF(d,t): DF(t)에 반비례하는 수
- TF(d,t)*IDF(d,t) = TF-IDF(d,t)
- 장점: 직관적인 해석이 가능함
- 단점: 대규모 말뭉치를 다룰 때 메모리상의 문제가 발생
	- 높은 차원을 가짐
	- 매우 sparse한 형태의 데이터임

### Word2Vec
- 통계기반의 방법 단점
	- 대규모 말뭉치를 다룰 때 메모리 상의 문제가 발생
		- 높은 차원을 가짐, 매우 sparse한 형태의 데이터임
		- 예) 100만개의 문서를 다루는 경우 : 100만개의 문서에 등장한 모든 단어를 추출해야 하고 이때 단어의 수는 1 문서당 새로운 단어가 10개면, 1000만개 정도의 말뭉치가 형성됨. 즉, 100만 x 1000만의 매트릭스가 형성
	- 한번에 학습 데이터 전체를 진행함
		- 큰 작업을 처리하기 어려움
		- GPU와 같은 병렬처리를 기대하기 힘듬
	- 학습을 통해서 개선하기가 어려움
- 추론기반의 방법
	- 추론: 주변 단어(맥락)이 주어졌을 때 "?"에 무슨 단어(중심단어)가 들어가는지를 추측하는 작업
	- CBOW 모델
		-  주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법
		- 원리
			1. One Hot Vector 형태의 입력값을 받는다(2개)
			2. One Hot Vector 형태의 입력값을 Win과 곱(차원의 크기는 사용자가 선정)
			3. Hidden state의 값을 W_out과 곱해서 Score를 추출한다
			4. Score에 Softmax를 취해서 각 단어가 나올 확률을 계산한다
			5. 정답과 Cross Entropy Loss를 계산
			6. 5에서 계산한 Loss를 가지고 Backpropagation 과정을 통해서 Weight를 업데이트
			7. 위의 과정을 다른 문맥에 대해서도 수행 
	- skip-gram 모델
		- 중간에 있는 단어로 주변 단어들을 예측하는 방법
		- 원리
			1. One Hot Vector 형태의 입력값을 받는다(1개)
			2. One Hot Vector 형태의 입력값을 Win과 곱
			3. Hidden state의 값을 W_out과 곱해서 Score를 추출한다
			4. Score에 Softmax를 취해서 각 단어가 나올 확률을 계산한다
			5. 정답과 Cross Entropy Loss를 계산
			6. 5에서 계산한 Loss를 가지고 Backpropagation 과정을 통해서 Weight를 업데이트
			7. 위의 과정을 다른 문맥에 대해서도 수행

#### 정의
 - Word2Vec은 단어간 유사도를 반영하여 단어를 벡터로 바꿔주는 임베딩 방법론입니다. 
 - 원-핫벡터 형태의 sparse matrix이 가지는 단점을 해소하고자 저차원의 공간에 벡터로 매핑하는 것이 특징입니다.
 - Word2Vec은 "비슷한 위치에 등장하는 단어들은 비슷한 의미를 가진다" 라는 가정을 통해서 학습을 진행합니다.
 - 저차원에 학습된 단어의 의미를 분산하여 표현하기에 단어 간 유사도를 계산할 수 있습니다.
 - 추천시스템에서는 단어를 구매 상품으로 바꿔서 구매한 패턴에 Word2Vec을 적용해서 비슷한 상품을 찾을 수 있습니다.

### 협업필터링
- 협업필터링은 사용자의 구매 패턴이나 평점을 가지고 다른 사람들의 구매 패턴, 평점을 통해서 추천을 하는 방법
- 추가적인 사용자의 개인정보나 아이템의 정보가 없이도 추천할 수 있는게 큰 장점
- 최근접 이웃기반, 잠재 요인기반
- 장점
	- 도메인 지식이 필요하지 않음
	- 사용자의 새로운 흥미를 발견하기 좋음
	- 시작단계의 모델로 선택하기 좋음(추가적인 문맥정보 등의 필요가 없음)
- 단점
	- 새로운 아이템에 대해서 다루기가 힘듬
	- side features(고객의 개인정보, 아이템의 추가정보)를 포함시키기 어려움

#### Neighborhood based method
- Neighborhood based Collaborative Filtering은 메모리 기반 알고리즘으로 협업 필터링을 위해 개발된 초기 알고리즘
- User-based collaborative filtering
	- 사용자의 구매 패턴(평점)과 유사한 사용자를 찾아서 추천 리스트 생성
- Item-based collaborative filtering
	- 특정 사용자가 준 점수간의 유사한 상품을 찾아서 추천 리스트 생성
- 장점
	- 간단하고 직관적인 접근 방식 때문에 구현 및 디버그가 쉬움
	- 특정 Item을 추천하는 이유를 정당화하기 쉽고 Item 기반 방법의 해석 가능성이 두드러짐
	- 추천 리스트에 새로운 item과 user가 추가되어도 상대적으로 안정적
- 단점
	- User 기반 방법의 시간, 속도, 메모리가 많이 필요
	- 희소성 때문에 제한된 범위가 있음
		- John의 Top-K에만 관심이 있음
		- John과 비슷한 이웃중에서 아무도 해리포터를 평가하지 않으면, John의 해리포터에 대한 등급 예측을 제공할 수가 없음
##### K Nearest Neighbors
- 가장 근접한 K명의 Neighbors를 통해서 예측하는 방법

#### Latent Factor Collaborative Filtering
- 잠재 요인 협업 필터링은 Rating Matrix에서 빈 공간을 채우기 위해서 사용자와 상품을 잘 표현되는 차원(Latent Factor)을 찾는 방법
- 잘 알려진 행렬 분해는 추천 시스템에서 사용되는 협업 필터링 알고리즘의 한 종류
- 행렬 분해 알고리즘은 사용자-아이템 상호 작용 행렬을 두 개의 저차원 직사각형 행렬의 곱으로 분해하여 작동
- SVD, SGD, ALS
###### SGD
- 고유값 분해(eigen value Decomposition)와 같은 행렬을 대각화 하는 방법
- 장점
	- 매우 유연한 모델로 다른 Loss function을 사용할 수 있음
	- parallelized가 가능함
- 단점
	- 수렴까지 속도가 매우 느림
##### ALS
- 기존의 SGD가 두개의 행렬(User Latent, Item Latent)을 동시에 최적화하는 방법이라면, ALS는 두 행렬 중 하나를  고정시키고 다른 하나의 행렬을 순차적으로 반복하면서 최적화 하는 방법
- 이렇게 하면, 기존의 최적화 문제가 convex 형태로 바뀌기에 수렴된 행렬을 찾을 수 있는 장점
- 알고리즘
	1. 초기 아이템, 사용자 행렬을 초기화
	2. 아이템 행렬을 고정하고 사용자 행렬을 최적화
	3. 사용자 행렬을 고정하고 아이템 행렬을 최적화
	4. 위의 2, 3 과정을 반복
- 장점
	- SGD보다 수렴 속도가 빠름
	- parallelized가 가능함
- 단점
	- 오직 Loss Squares만 사용 가능

### 평가함수
- 평가함수는 추천시스템의 모델을 생성하고 해당 모델이 얼마나 잘 추천하고 있는지에 대해서 평가를 도와주는 함수
- 도메인이나 목적에 따라서 다른 평가 함수를 도입해서 얼마나 잘 추천이 되는지 평가하는게 중요
- 영화 평점의 경우에서는 두 가지 형태로 평가 가능
	- 내가 추천해준 영화를 고객이 봤나?
	- 내가 추천해준 영화를 고객이 높은 점수로 평점을 줬나?
#### 정확도
- 내가 추천해준 영화를 고객이 봤나? vs 보지 않았나?
- 내가 추천해주는 영화를 많이 볼수록 추천하지 않은 영화를 보지 않을 수록 정확도는 상승
- 하지만, 추천하지 않은 영화의 수는 추천한 영화의 수에 비해 굉장히 많고 편향된 결과를 얻을 수 있음
#### MAP
- Recommendations: 추천을 했는데 맞은 경우 1, 틀리면 0
- AP: Precision @k's를 평균낸 값(추천한 k개의 영화의 Precision을 평균)
- MAP@4: 4명의 사용자의 AP를 평균낸 값(Precision을 평균낸 AP를 4명의 사용자에 대해 평균)
- MAP의 경우 추천의 순서에 따라서 값이 차이가 납니다. 도한, 상위 k개의 추천에 대해서만 평가하기에 k를 바꿔가면서 상위 몇 개를 추천하는게 좋을지도 결정할 수 있습니다.
#### NDCG(Normalized Discounted Cumulative Gain)
- ranking quality measure, 검색 알고리즘에서 성과를 측정하는 평가 메트릭입니다.
- 추천엔진은 user와 연관있는 documents의 집합을 추천해주기 때문에, 단순히 문서 검색 작업을 수행한다고 생각할 수 있습니다. (검색과 관련있는 문서들을 추천) 따라서 NDCG를 사용하여 추천엔진을 평가할 수 있습니다. NDCG를 이해하기 위해서는 Cumulative Gain과 Discounted Cumulative Gain을 이해할 필요가 있습니다.
